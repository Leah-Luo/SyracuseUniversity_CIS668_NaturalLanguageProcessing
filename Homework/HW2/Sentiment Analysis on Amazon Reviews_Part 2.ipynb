{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import FreqDist\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import stopwords\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk.tree import Tree\n",
    "import time\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a great tutu\n"
     ]
    }
   ],
   "source": [
    "# Open the review file \n",
    "textfile = open(\"/Users/LXIN/Desktop/reviews.txt\")\n",
    "reviewText = textfile.read()\n",
    "print(reviewText[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140642\n",
      "This is a great tutu and at a really great price.\n",
      "It doesn't look cheap at all.\n",
      "I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly.\n",
      "A++\n",
      "I bought this for my 4 yr old daughter for dance class, she wore it today for the first time and the teacher thought it was adorable.\n",
      "I bought this to go with a light blue long sleeve leotard and was happy the colors matched up great.\n",
      "Price was very good too since some of these go for over $15.00 dollars.\n",
      "What can I say... my daughters have it in orange, black, white and pink and I am thinking to buy for they the fuccia one.\n",
      "It is a very good way for exalt a dancer outfit: great colors, comfortable, looks great, easy to wear, durables and little girls love it.\n",
      "I think it is a great buy for costumer and play too.\n",
      "We bought several tutus at once, and they are got high reviews.\n",
      "Sturdy and seemingly well-made.\n",
      "The girls have been wearing them regularly, including out to play, and the tutus have stood up well.\n",
      "Fits the 3-yr old & the 5-yr old well.\n",
      "Clearly plenty of room to grow.\n",
      "Only con is that when the kids pull off the tutus, the waste band gets twisted, and an adult has to un-tangle.\n",
      "But this is not difficult.\n",
      "Thank you Halo Heaven great product for Little Girls.\n",
      "My Great Grand Daughters Love these Tutu's.\n",
      "Will buy more from this seller.\n",
      "Made well and cute on the girls.\n",
      "Thanks for a great product.NEVER BUY FROM DRESS UP DREAMS........I will buy more as long as I don't buy from &#34;Dress Up Dreams&#34;  I never rec'd or order in FL.\n",
      "Only rec'd pink, the purple one was missing.\n",
      "Company is a rip off.\n",
      "REFUSES to make good on purchase...... Real creeps.\n",
      "I received this today and I'm not a fan of it but my daughter is I thought it would be puffier as it looks in the pic but it's not and the one they sent me is pink underneath and the waist band is pink which is not what I wanted due to the fact she already had the sandals she was gonna wear with it now I gotta find another pair of sandals,ima just keep it cuz she likes it.\n",
      "Bought this as a backup to the regular ballet outfit my daughter has to wear.\n",
      "So far, she's using it to play out her Cinderella dreams but I am sure we'll be able to use it for a recital sometime soon.\n",
      "The quality is just fine for the price we paid.\n",
      "I was not expecting a designer skirt for this price and got exactly what I paid for.\n",
      "Great tutu for a great price.\n",
      "It isn't a &#34;full&#34; or high quality skirt, but it is perfect for my daughter to wear over leggings for her little outfits.\n",
      "My daughter liked this, and it with her costume, but she would have liked it to be a bit fuller.\n",
      "For what I paid for two tutus is unbeatable anywhere!\n",
      "I ordered a pink and turquios and they are vibrant and beautiful!\n",
      "The tutu is very full!\n",
      "Princess style!\n",
      "Not cheaply made!\n",
      "Not cheap materia!\n",
      "Obviously someone made these with love and care!\n",
      "I paid less than 7 bucks for a tutu I and I feel proud of my self for researching to the point of finding gold!Recommend 2-6 years!My daughter is two !\n",
      "Wears size 4t and this skirt ( one size ) fit perfect and will probaly be able to accommodate her quickly growing waist for some time!\n",
      "Wonder my niece wears it every single day, yellow is her favorite color right now an this cute little tutu made he da.\n",
      "It is well built and we hope she gets lots of wear out of it.\n",
      "My daughter has worn this skirt almost every day since she received it and it's even been through the washer along with the other clothes.\n",
      "It's amazing quality!\n",
      "She fits a 4T and it's just above her knee, and has a little bit of growing room, although I'm not so sure as much as others are saying.\n",
      "But considering how often she wears it, I'm not worried!\n",
      ";)\n",
      "Purchased this tutu for my granddaughter's first birthday.\n",
      "It fit nicely with room to grow in.\n",
      "I'm very please with it and will purchase more in varying colors in the future.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "from nltk import tokenize\n",
    "tokensen = tokenize.sent_tokenize(reviewText)\n",
    "print(len(tokensen))\n",
    "for s in tokensen[:50]:\n",
    "\tprint(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentences \n",
    "startime = time.time()\n",
    "nlp = StanfordCoreNLP('/Users/LXIN/Desktop/stanford-corenlp-full-2018-10-05', timeout = 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = []\n",
    "im = []\n",
    "imperative = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuunctions for extracting \n",
    "def filt(x):\n",
    "\treturn x.label()=='SQ' or x.label()=='SBARQ'\n",
    "def f2(x):\n",
    "\treturn x.label() == 'JJ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract question sentences  \n",
    "for sentence in tokensen:\t\n",
    "\tif sentence[len(sentence)-1] == '?':\n",
    "\t\ta = (nlp.parse(sentence))\n",
    "\t\tparsetree = Tree.fromstring(a)\n",
    "\t\tfor subtree in parsetree.subtrees(filter =  filt):\n",
    "\t\t\tfor subtree in parsetree.subtrees(filter =  f2):\n",
    "\t\t\t\tques.append(sentence)\n",
    "\t\t\t\tbreak\n",
    "\t\t\tbreak\n",
    "\t\t# Store the rest sentence in a new list for extracting imperative sentence later\n",
    "\telif sentence[len(sentence)-1] == '!':\n",
    "\t\tim.append(sentence)\n",
    "# First few question sentences \n",
    "print('Length of question sentences: ',len(ques))\n",
    "for s in ques[:30]:\n",
    "\tprint(s)\n",
    "print('\\n-----------------------------\\n')\n",
    "print('\\nTime: ',(time.time()-startime))\n",
    "\n",
    "nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract imperative sentences \n",
    "startime = time.time()\n",
    "im2 = []\n",
    "\n",
    "rmVerb = ['Love','Will','Recommend','Want','Wonder','Seems']\n",
    "rmVerbLow = ['love','will','recommend','want','wonder','seems']\n",
    "titleWord = ['Always', 'Never', 'Please', 'Just']\n",
    "\n",
    "# Include only the sentence with adj phrases \n",
    "for s in im:  \n",
    "\ttokenword = nltk.word_tokenize(s)  \n",
    "\ttaggedtextStanford = nltk.pos_tag(tokenword)\n",
    "\tfor word in taggedtextStanford:\n",
    "\t\tif word[1] == 'JJ':    \n",
    "\t\t\tim2.append(s)\n",
    "\t\t\tbreak\n",
    "\t\t\t\n",
    "print(len(im2))\n",
    "print(im2[:30])\n",
    "print('\\n-----------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Imperative sentences \n",
    "for s in im2:\n",
    "\ttokenword = nltk.word_tokenize(s)  \n",
    "\ttaggedtextStanford = nltk.pos_tag(tokenword)\n",
    "\tif taggedtextStanford[0][0] in titleWord:\n",
    "\t\tif taggedtextStanford[1][1] == 'VB' and taggedtextStanford[1][0] not in rmVerbLow:\n",
    "\t\t\timperative.append(s)\n",
    "\tif taggedtextStanford[0][1] == 'VB':\n",
    "\t\tif taggedtextStanford[0][0] not in rmVerb: \n",
    "\t\t\timperative.append(s)\n",
    "\n",
    "print('Length of imperative sentences: ',len(imperative))\n",
    "for s in imperative[:100]:\n",
    "\tprint(s)\n",
    "print('\\nTime: ',(time.time()-startime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the table \n",
    "lengthsQ = [len(i) for i in ques]\n",
    "avgQ = sum(lengthsQ) / len(ques)\n",
    "\n",
    "lengthsI = [len(i) for i in imperative]\n",
    "avgI = sum(lengthsI) / len(imperative)\n",
    "\n",
    "t = PrettyTable(['Type', 'Number of sentences', 'Average length of sentence'])\n",
    "t.add_row(['Question sentence', len(ques), avgQ])\n",
    "t.add_row(['Imperative sentence', len(imperative), avgI])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Sentence Analysis \n",
    "# Question Sentences -- Unigram analysis\n",
    "tokenQues = []\n",
    "for s in ques:\n",
    "\ttokenQues.append(nltk.word_tokenize(s))\n",
    "\n",
    "# Make a flat list out of list of lists(tokenQues)\n",
    "tokenQues2 = [item for sublist in tokenQues for item in sublist]\n",
    "# Convert all the alphabetical characters to lower case\n",
    "alphaWords = [w.lower() for w in tokenQues2 if w.isalpha()]\n",
    "print(alphaWords[:20])\n",
    "# Remove stop words for unigram\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "rmStop = [w for w in alphaWords if not w in stopwords]\n",
    "# Lemmatization\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "lem = []\n",
    "for rmStop, tag in pos_tag(rmStop):\n",
    "\tlemma = wnl.lemmatize(rmStop, tag_map[tag[0]])\n",
    "\tlem.append(lemma)\n",
    "print(lem[:20])\n",
    "print('\\n-------------------------\\n')\n",
    "# Frequency distribution\n",
    "fdist = FreqDist(lem)\n",
    "fdistKey = list(fdist.keys())\n",
    "topkey = fdist.most_common(50)\n",
    "for pair in topkey:\n",
    "\tprint(pair)\n",
    "print('\\n-------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Sentences -- Bigram analysis\n",
    "from nltk.collocations import *\n",
    "def alpha_filter(w):\n",
    "\tpattern = re.compile('^[^a-z]+$')\n",
    "\tif(pattern.match(w)):\n",
    "\t\treturn True\n",
    "\telse:\n",
    "\t\treturn False\n",
    "\t\n",
    "lowercase = [w.lower() for w in tokenQues2] \n",
    "print('\\nBigram-------------------------')\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(lowercase)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "first = scored[0]\n",
    "for bscore in scored[:20]:\n",
    "\tprint(bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha bigrams\n",
    "print('\\nAlpha Bigrams-------------------------')\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "scored1 = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored1[:50]:\n",
    "\tprint(bscore)\n",
    "\n",
    "print('\\nStopword Bigrams-------------------------')  \n",
    "# stopword bigrams  -> Bigrams Result\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "scored2 = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored2[:50]:\n",
    "\tprint(bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imperative Sentence Analysis \n",
    "# imperative Sentences -- Unigram analysis\n",
    "tokenIm = []\n",
    "for s in imperative:\n",
    "\ttokenIm.append(nltk.word_tokenize(s))\n",
    "\t\n",
    "# Make a flat list out of list of lists(tokenQues)\n",
    "tokenIm2 = [item for sublist in tokenIm for item in sublist]\n",
    "# Convert all the alphabetical characters to lower case\n",
    "alphaWords = [w.lower() for w in tokenIm2 if w.isalpha()]\n",
    "print(alphaWords[:20])\n",
    "# Remove stop words for unigram\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "rmStop = [w for w in alphaWords if not w in stopwords]\n",
    "# Lemmatization\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "lem = []\n",
    "for rmStop, tag in pos_tag(rmStop):\n",
    "\tlemma = wnl.lemmatize(rmStop, tag_map[tag[0]])\n",
    "\tlem.append(lemma)\n",
    "print(lem[:20])\n",
    "print('\\n-------------------------\\n')\n",
    "# Frequency distribution\n",
    "fdist = FreqDist(lem)\n",
    "fdistKey = list(fdist.keys())\n",
    "topkey = fdist.most_common(50)\n",
    "for pair in topkey:\n",
    "\tprint(pair)\n",
    "print('\\n-------------------------\\n')\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Sentences -- Bigram analysis\n",
    "from nltk.collocations import *\n",
    "def alpha_filter(w):\n",
    "\tpattern = re.compile('^[^a-z]+$')\n",
    "\tif(pattern.match(w)):\n",
    "\t\treturn True\n",
    "\telse:\n",
    "\t\treturn False\n",
    "\t\n",
    "lowercase = [w.lower() for w in tokenIm2] \n",
    "print('\\nBigram-------------------------')\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(lowercase)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "first = scored[0]\n",
    "for bscore in scored[:20]:\n",
    "\tprint(bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha bigrams\n",
    "print('\\nAlpha Bigrams-------------------------')\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "scored1 = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored1[:50]:\n",
    "\tprint(bscore)\n",
    "\n",
    "print('\\nStopword Bigrams-------------------------')  \n",
    "# stopword bigrams  -> Bigrams Result\n",
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "scored2 = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored2[:50]:\n",
    "\tprint(bscore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
